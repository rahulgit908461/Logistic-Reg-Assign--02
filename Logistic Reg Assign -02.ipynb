{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66497d2-4976-493c-85fc-1a91675ab64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b560f9b-0a9f-4eeb-ac88-1e8e1b819290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each \n",
    "#combination using the Cross-Validation method.\n",
    "#Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose \n",
    "#the one with the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c5e367-441f-488f-83b1-fba9debe44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "#one over the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3d5ad46-4f13-4aac-874d-8b2f65e684cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in grid search we define the combinations and do training of the model whereas in RandomizedSearchCV the model \n",
    "#selects the combinations randomly.\n",
    "#Both are very effective ways of tuning the parameters that increase the model generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "149d6aae-3e3e-4946-9740-294f56f097ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a259e2-c0b7-4877-ad10-74c5ce3e675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One of the best ways to get rid of data leakage is to perform k-fold cross validation where the overall \n",
    "#data is divided into k parts. After dividing into k parts, we use each part as the cross-validation data and \n",
    "#the remaining as training data.\n",
    "#when sensitive data gets unintentionally exposed to the public in transit, at rest, or in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9696fe5-7fee-4cca-9b64-c8a516fce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b53ee0d-750d-4c1b-9fbc-2e9d6b69bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform k-fold cross validation where the overall data is divided into k parts. After dividing into k parts, \n",
    "#we use each part as the cross-validation data and the remaining as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4de89f6-4946-4516-acde-862dad7597f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b67d5b9-4b2f-45f5-bca3-2e9cd4d14afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A confusion matrix is a table that allows you to visualize the performance of a classification model. You can \n",
    "#also use the information in it to calculate measures that can help you determine the usefulness of the model.\n",
    "#Rows represent predicted classifications, while columns represent the true classes from the data.\n",
    "##a summarized table used to assess the performance of a classification model. The number of correct and \n",
    "#incorrect predictions are summarized with count values and broken down by each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a71f9277-84cc-4753-a673-9c00edac5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3cf5452-7527-44ac-b403-7c2b39614314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision (also called positive predictive value) is the fraction of relevant instances among the retrieved\n",
    "#instances, while recall (also known as sensitivity) is the fraction of relevant instances that were retrieved.\n",
    "#Both precision and recall are therefore based on relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b135e22-bccf-4e88-91d7-e549b5a500f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375501d7-18f2-4bb6-82fc-1189b082996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy (all correct / all) = TP + TN / TP + TN + FP + FN.\n",
    "#Misclassification (all incorrect / all) = FP + FN / TP + TN + FP + FN.\n",
    "#Precision (true positives / predicted positives) = TP / TP + FP.\n",
    "#Sensitivity aka Recall (true positives / all actual positives) = TP / TP + FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5030631f-75a1-4c22-9ced-8c7088a7b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "#calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f236be99-e605-49a8-9166-3c45697df5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrices can be used to calculate performance metrics for classification models. Of the many\n",
    "#performance metrics used, the most common are accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e67c0d30-4027-40c6-b91b-4c4457c4cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea503dc8-b453-4d3d-94ac-0b4e9c88fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You may achieve accuracy of 90% or more, but this is not a good score if 90 records for every 100 belong \n",
    "#to one class and you can achieve this score by always predicting the most common class value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ded7987-682e-449d-bab8-1de7f2dd5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "#model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d6287-56df-42a7-8371-16aeaab75ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A confusion matrix presents a table layout of the different outcomes of the prediction and results of a \n",
    "classification problem and helps visualize its outcomes. It plots a table of all the predicted and actual values of a classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
